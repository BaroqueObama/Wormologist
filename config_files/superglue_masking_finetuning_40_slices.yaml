experiment:
  experiment_name: "one_to_one_sliced_data_superglue_finetuned_top10" # to modify for each experiment
  wandb_project: "celegans"
  wandb_tags: ["accurate", "muon", "yaml", "no_curr", "rot_invar"]
  log_interval: 10
  save_top_k: 3
  monitor_metric: "val/accuracy"
  monitor_mode: "max"

model:
  hidden_dim: 256
  num_heads: 8
  num_layers: 8
  
  use_mla_layers: [4, 5, 6, 7]  # Which layers use MLA
  mla_kv_latent_dim: 64       # KV compression (256→64)

  edge_correction_dim_ratio: 0.5  # Edge attention overhead
  learnable_edge_weight: true
  initial_edge_weight: 0.5

  ffn_dim_multiplier: 4.0

  norm_style: "pre_post"

  k_hops: 31
  update_edge_repr: true

  use_multiscale_pe: false  # Enable multi-scale RRWP filtration
  num_scales: 4  # Number of different scales for filtration
  min_sigma: 0.1  # Minimum sigma for Gaussian kernel (very local)
  max_sigma: 0.7  # Maximum sigma for Gaussian kernel (regional)
  learnable_scales: true

  attn_dropout: 0.0
  dropout: 0.0

  use_degree_scaler: false

  out_dim: 558


training:
  batch_size: 32
  micro_batch_size: 2 # Adjust size to fit in GPU memory
  epochs: 1
  gradient_clip_val: 1.0
  val_check_interval: 64
  limit_val_batches: 64
  checkpoint_interval: 64
  seed: 42


optimizer:
  optimizer_type: "muon"
  
  # Muon-specific parameters for 2D weight matrices
  muon_lr: 0.003  # Learning rate for Muon parameters
  muon_momentum: 0.95  # Momentum coefficient for Muon
  muon_nesterov: true  # Use Nesterov momentum
  muon_ns_steps: 5  # Newton-Schulz iteration steps for orthogonalization
  
  # AdamW parameters for non-2D params (embeddings, biases, layer norms)
  muon_adamw_lr: 0.003  # Learning rate for AdamW parameters when using Muon
  muon_adamw_betas: [0.9, 0.95]  # Betas for AdamW optimizer
  muon_adamw_eps: 1.0e-8  # Epsilon for AdamW optimizer
  muon_adamw_wd: 0.01  # Weight decay for AdamW optimizer
  
  # Fallback parameters (used if switching to pure AdamW)
  learning_rate: 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8
  momentum: 0.9


scheduler:
  scheduler_type: "cosine"  # "cosine", "linear", "constant"
  warmup_batches: 128           # Warmup for 512 batches
  warmup_start_lr: 1.0e-6       # Start from 1e-6
  min_lr: 0.0005              # End at after cosine decay
  total_training_batches: 8192 # 16384


sinkhorn:
  num_iterations: 100
  init_temperature: 1.0
  final_temperature: 1.0
  temperature_warmup_steps: 0
  dustbin_weight: 1.0
  epsilon: 1e-8
  one_to_one: true

loss:
  class_weight: 1.0
  assignment_weight: 1.0
  label_smoothing: 0.1
  worm_averaged_loss: true
  use_superglue_loss: true
  use_topk_sinkhorn_mask: true
  topk_sinkhorn_k: 10


augmentation:
  enabled: true
  z_shift_range: [0.005, 0.035]
  z_shift_distribution: "beta"  # "uniform" or "beta"
  z_shift_beta_alpha: 1.0  # Beta distribution shape parameter
  z_shift_beta_beta: 1.0   # Beta distribution shape parameter

  # Uniform scaling parameters (applied to all axes before rotation)
  uniform_scale_range: [0.88, 1.05]
  uniform_scale_distribution: "beta"  # "uniform" or "beta"
  uniform_scale_beta_alpha: 1.0  # Beta distribution shape parameter
  uniform_scale_beta_beta: 1.0   # Beta distribution shape parameter
  
  # Rotation range for z-axis in degrees
  z_rotation_range: [-10.0, 10.0]
  rotation_distribution: "uniform"  # "uniform" or "beta" for rotation
  rotation_beta_alpha: 1.0  # Beta distribution shape parameter for rotation
  rotation_beta_beta: 1.0   # Beta distribution shape parameter for rotation

  # Post-rotation x-axis scaling (y-axis scales by 1/x_scale to preserve area)
  post_rotation_xy_scale_range: [0.8, 1.2]
  post_rotation_xy_scale_distribution: "beta"  # "uniform" or "beta"
  post_rotation_xy_scale_beta_alpha: 1.2  # Beta distribution shape parameter
  post_rotation_xy_scale_beta_beta: 1.2   # Beta distribution shape parameter
  # Random orientation for xy scaling (in degrees, 0-360)
  xy_scale_orientation_range: [0.0, 360.0]

  apply_to_splits: ["train", "val"]
  seed: 42


curriculum:
  enabled: true
  subgraph_strategy: sliced
  slice_max_n_slices: 45

  # No real curriculum – force "uniform" phase in your Config/get_phase
  warmup_batches: 0
  curriculum_batches: 0
  cooldown_batches: 0

  # Visibility range corresponding to 35–45 slices
  start_visibility: 1.0          # 45 / 45
  end_visibility: 0.7778  # 35 / 45

  train_distribution: uniform    # ignored in the "uniform" phase branch

  # Keep your slice_profiles and other fields as is
  slice_profiles:
    - {thickness_fraction: 0.007, crop_fraction_range: [0.5, 1.0]}
    - {thickness_fraction: 0.008, crop_fraction_range: [0.0, 1.0]}
    - {thickness_fraction: 0.009, crop_fraction_range: [0.0, 0.5]}
  slice_crop_axis: random
  slice_seed: 42

  # For val: fix around the same range if you want
  val_min_visibility: 0.7778
  val_max_visibility: 1.0



data:
  data_path: "/fs/pool/pool-mlsb/bulat/Wormologist/data_finetune/finetune" # sometimes modify
  coordinate_system: "cylindrical"  # "cartesian" or "cylindrical"
  normalize_coords: false  # Disabled to avoid issues with subgraphs
  use_cell_type_features: true  # Include one-hot encoded cell type features
  num_workers: 0
  pin_memory: true


graph:
  distance_kernel: "inverse"
  kernel_sigma: 0.5
  use_random_walks: true
  walk_length: 31


system:
  device: "cuda"
  mixed_precision: "bf16-mixed"  # "bf16", "fp16", "fp32"
  distributed: true
  num_gpus: 4

storage:
  # Custom storage directories
  checkpoint_dir: "/fs/pool/pool-mlsb/bulat/Wormologist/model_checkpoints/checkpoint_one_to_one_sliced_data_superglue_finetuned_top10"  # modify for each experiment
  wandb_dir: "/fs/pool/pool-mlsb/bulat/Wormologist/wandb_one_to_one_sliced_data_superglue_finetuned_top10"  # modify for each experiment
  use_experiment_subdir: true  # Create subdirectory with experiment name
  save_config_copy: true  # Save a copy of the config with checkpoints
