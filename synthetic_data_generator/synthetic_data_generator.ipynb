{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import List, Union, Tuple\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_joint(mu: np.ndarray, Sigma: np.ndarray, n_samples: int, rng=None):\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    N, D = mu.shape\n",
    "    vec = rng.multivariate_normal(mu.ravel(), Sigma, size=n_samples, method=\"cholesky\")\n",
    "    return vec.reshape(n_samples, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sigma.pkl\", \"rb\") as f:\n",
    "    Sigma = pickle.load(f)\n",
    "\n",
    "with open(\"mu.pkl\", \"rb\") as f:\n",
    "    mu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8083978",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path(\"/fs/gpfs41/lv11/fileset01/pool/pool-smola/pythonny/data/long_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2541837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sample_from_joint(mu, Sigma, n_samples=10*(2**15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3578f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5_file(specimens_data: List[np.ndarray], output_path: Path, file_prefix: str = \"specimens\", start_idx: int = 0) -> int:\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        specimens_group = f.create_group('specimens')\n",
    "        \n",
    "        for i, specimen_data in enumerate(specimens_data):\n",
    "            specimen_key = f\"{file_prefix}_{start_idx + i:06d}\"\n",
    "            \n",
    "            if specimen_data.shape != (558, 3):\n",
    "                raise ValueError(f\"Expected shape (558, 3), got {specimen_data.shape} for specimen {i}\")\n",
    "            \n",
    "            canonical_ids = np.arange(558).reshape(-1, 1)\n",
    "            specimen_full = np.hstack([canonical_ids, specimen_data])\n",
    "            \n",
    "            specimens_group.create_dataset(specimen_key, data=specimen_full)\n",
    "        \n",
    "        f.attrs['num_specimens'] = len(specimens_data)\n",
    "        f.attrs['format_version'] = '1.1'\n",
    "        f.attrs['description'] = 'C. elegans nuclei data: [canonical_id, x, y, z]'\n",
    "    \n",
    "    return start_idx + len(specimens_data)\n",
    "\n",
    "\n",
    "def convert_specimens_to_hdf5(specimens_data: List[np.ndarray], output_dir: Union[str, Path], split_ratios: Tuple[float, float, float] = (0.8, 0.10, 0.10), specimens_per_file: int = 2**14, shuffle: bool = True) -> None:\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if abs(sum(split_ratios) - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"Split ratios must sum to 1.0, got {sum(split_ratios)}\")\n",
    "    \n",
    "    print(f\"Converting {len(specimens_data)} specimens to HDF5...\")\n",
    "    print(f\"Split ratios: train={split_ratios[0]:.1%}, val={split_ratios[1]:.1%}, test={split_ratios[2]:.1%}\")\n",
    "    print(f\"Specimens per file: {specimens_per_file}\")\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(specimens_data))\n",
    "        specimens_data = [specimens_data[i] for i in indices]\n",
    "        print(\"Data shuffled\")\n",
    "    \n",
    "    n_total = len(specimens_data)\n",
    "    n_train = int(n_total * split_ratios[0])\n",
    "    n_val = int(n_total * split_ratios[1])\n",
    "    n_test = n_total - n_train - n_val\n",
    "    \n",
    "    print(f\"Split sizes: train={n_train}, val={n_val}, test={n_test}\")\n",
    "    \n",
    "    train_data = specimens_data[:n_train]\n",
    "    val_data = specimens_data[n_train:n_train + n_val]\n",
    "    test_data = specimens_data[n_train + n_val:]\n",
    "\n",
    "    splits = [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]\n",
    "\n",
    "    for split_name, split_data in splits:\n",
    "        if not split_data:\n",
    "            print(f\"Warning: No data for {split_name} split\")\n",
    "            continue\n",
    "        \n",
    "        split_dir = output_dir / split_name\n",
    "        split_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        file_idx = 0\n",
    "        specimen_idx = 0\n",
    "        \n",
    "        for start_idx in range(0, len(split_data), specimens_per_file):\n",
    "            end_idx = min(start_idx + specimens_per_file, len(split_data))\n",
    "            batch_data = split_data[start_idx:end_idx]\n",
    "            \n",
    "            file_path = split_dir / f\"{split_name}_{file_idx:04d}.h5\"\n",
    "            next_specimen_idx = create_hdf5_file(batch_data, file_path, file_prefix=\"specimen\", start_idx=specimen_idx)\n",
    "\n",
    "            print(f\"  Created {file_path} with {len(batch_data)} specimens\")\n",
    "            \n",
    "            file_idx += 1\n",
    "            specimen_idx = next_specimen_idx\n",
    "    \n",
    "    info = {\n",
    "        \"total_specimens\": n_total,\n",
    "        \"splits\": {\n",
    "            \"train\": n_train,\n",
    "            \"val\": n_val, \n",
    "            \"test\": n_test\n",
    "        },\n",
    "        \"specimens_per_file\": specimens_per_file,\n",
    "        \"format\": \"[canonical_id, x, y, z]\",\n",
    "        \"canonical_id_range\": \"[0, 557]\"\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / \"dataset_info.json\", 'w') as f:\n",
    "        json.dump(info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset created in {output_dir}\")\n",
    "    print(f\"Dataset info saved to {output_dir / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bcdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_specimens_to_hdf5(sims, data_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
