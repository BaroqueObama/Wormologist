{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3179ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence, Union, Tuple, Dict, Any, Optional\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key difference here is that we remove canonical_ids = np.arange(558).reshape(-1, 1)\n",
    "def create_hdf5_file(\n",
    "    specimens_data: List[np.ndarray],\n",
    "    output_path: Path,\n",
    "    file_prefix: str = \"specimen\",\n",
    "    start_idx: int = 0,\n",
    ") -> int:\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with h5py.File(output_path, \"w\") as f:\n",
    "        specimens_group = f.create_group(\"specimens\")\n",
    "\n",
    "        for i, specimen_data in enumerate(specimens_data):\n",
    "            specimen_key = f\"{file_prefix}_{start_idx + i:06d}\"\n",
    "            specimens_group.create_dataset(specimen_key, data=specimen_data.astype(np.float32))\n",
    "\n",
    "        f.attrs[\"num_specimens\"] = len(specimens_data)\n",
    "        f.attrs[\"format_version\"] = \"1.1\"\n",
    "        f.attrs[\"description\"] = \"C. elegans nuclei data: [canonical_id, x, y, z]\"\n",
    "\n",
    "    return start_idx + len(specimens_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33036f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we do not have the shuffling and splitting of the dataset because it all refers to the test set\n",
    "def convert_specimens_to_hdf5(\n",
    "    specimens_data: Union[np.ndarray, Sequence[np.ndarray]],\n",
    "    output_dir: Union[str, Path],\n",
    "    specimens_per_file: int = 2**14,\n",
    ") -> None:\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    specimens_list = list(specimens_data)\n",
    "\n",
    "    split_dir = output_dir / \"test\"\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    file_idx = 0\n",
    "    specimen_idx = 0\n",
    "\n",
    "    for start_idx in range(0, len(specimens_list), specimens_per_file):\n",
    "        end_idx = min(start_idx + specimens_per_file, len(specimens_list))\n",
    "        batch_data = specimens_list[start_idx:end_idx]\n",
    "\n",
    "        file_path = split_dir / f\"test_{file_idx:04d}.h5\"\n",
    "        specimen_idx = create_hdf5_file(\n",
    "            batch_data,\n",
    "            file_path,\n",
    "            file_prefix=\"specimen\",\n",
    "            start_idx=specimen_idx,\n",
    "        )\n",
    "        file_idx += 1\n",
    "\n",
    "    info = {\n",
    "        \"total_specimens\": len(specimens_list),\n",
    "        \"split\": \"test\",\n",
    "        \"specimens_per_file\": specimens_per_file,\n",
    "        \"format\": \"[canonical_id, x, y, z]\",\n",
    "    }\n",
    "\n",
    "    with open(output_dir / \"dataset_info.json\", \"w\") as f:\n",
    "        json.dump(info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef99221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_worm_1_pickle_path = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/synthetic_data_generator/test1worms.pkl\")\n",
    "test_worm_2_pickle_path = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/synthetic_data_generator/test2worms.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da296f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_worm_1_pickle_path, \"rb\") as f:\n",
    "    worm1 = pickle.load(f)\n",
    "with open(test_worm_2_pickle_path, \"rb\") as f:\n",
    "    worm2 = pickle.load(f)\n",
    "all_worms = worm1 + worm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23fe2f",
   "metadata": {},
   "source": [
    "# Initial random (with adapted file type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5db8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimens_per_file = 2**14\n",
    "rng_seed = 42\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "test_sizes = np.array(list(range(10, 560, 10)) + [558])\n",
    "subgraph_output_directory = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subgraph_size in test_sizes:\n",
    "    subgraph_samples: List[np.ndarray] = []\n",
    "\n",
    "    for answer, coords in all_worms:\n",
    "        nodes = len(answer)\n",
    "        sample_count = int(np.ceil(558 / subgraph_size)) * 2\n",
    "\n",
    "        for _ in range(sample_count):\n",
    "            sampled_indices = rng.choice(nodes, size=min(subgraph_size, nodes), replace=False)\n",
    "            canonical_ids = np.asarray(answer)[sampled_indices]\n",
    "            coords_subset = np.asarray(coords)[sampled_indices]\n",
    "\n",
    "            sample = np.zeros((len(sampled_indices), 4), dtype=np.float32)\n",
    "            sample[:, 0] = canonical_ids.astype(np.float32)\n",
    "            sample[:, 1:] = coords_subset.astype(np.float32)\n",
    "            subgraph_samples.append(sample)\n",
    "\n",
    "    subgraph_dir = subgraph_output_directory / f\"subgraph_{int(subgraph_size):03d}\"\n",
    "    \n",
    "    convert_specimens_to_hdf5(\n",
    "        subgraph_samples,\n",
    "        output_dir=subgraph_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(subgraph_samples)} subgraphs of size {subgraph_size} to {subgraph_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df77d",
   "metadata": {},
   "source": [
    "# Adapted random for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1694a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimens_per_file = 2**14\n",
    "rng_seed = 42\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "min_size = 53\n",
    "max_size = 59\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/random_comparison_to_real_test_set_20\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "sample_count = 12\n",
    "num_datasets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_idx in range(num_datasets):\n",
    "    subgraph_samples: List[np.ndarray] = []\n",
    "    subgraph_sizes: List[int] = []\n",
    "\n",
    "    for answer, coords in all_worms:\n",
    "        nodes = len(answer)\n",
    "        \n",
    "        for _ in range(sample_count):\n",
    "            subgraph_size = rng.integers(min_size, max_size + 1)\n",
    "            sample_size = min(subgraph_size, nodes)\n",
    "            subgraph_sizes.append(sample_size)\n",
    "            sampled_indices = rng.choice(nodes, size=sample_size, replace=False)\n",
    "\n",
    "            canonical_ids = np.asarray(answer)[sampled_indices]\n",
    "            coords_subset = np.asarray(coords)[sampled_indices]\n",
    "\n",
    "            sample = np.zeros((len(sampled_indices), 4), dtype=np.float32)\n",
    "            sample[:, 0] = canonical_ids.astype(np.float32)\n",
    "            sample[:, 1:] = coords_subset.astype(np.float32)\n",
    "            subgraph_samples.append(sample)\n",
    "\n",
    "    subgraph_dir = subgraph_output_directory / f\"dataset_{dataset_idx:02d}\"\n",
    "    \n",
    "    convert_specimens_to_hdf5(\n",
    "        subgraph_samples,\n",
    "        output_dir=subgraph_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "\n",
    "    avg_size = float(np.mean(subgraph_sizes)) if subgraph_sizes else float(\"nan\")\n",
    "    std_size = float(np.std(subgraph_sizes)) if subgraph_sizes else float(\"nan\")\n",
    "    min_size_obs = int(np.min(subgraph_sizes)) if subgraph_sizes else 0\n",
    "    max_size_obs = int(np.max(subgraph_sizes)) if subgraph_sizes else 0\n",
    "\n",
    "    print(\n",
    "        f\"Saved {len(subgraph_samples)} subgraphs \"\n",
    "        f\"(avg size {avg_size:.2f} ± {std_size:.2f}) to {subgraph_dir}\"\n",
    "    )\n",
    "\n",
    "    stats_path = subgraph_dir / \"subgraph_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"dataset_index: {dataset_idx}\\n\")\n",
    "        fh.write(f\"num_samples: {len(subgraph_samples)}\\n\")\n",
    "        fh.write(f\"size_range_config: [{min_size}, {max_size}]\\n\")\n",
    "        fh.write(f\"size_min_observed: {min_size_obs}\\n\")\n",
    "        fh.write(f\"size_max_observed: {max_size_obs}\\n\")\n",
    "        fh.write(f\"size_mean: {avg_size:.2f}\\n\")\n",
    "        fh.write(f\"size_std: {std_size:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cf73e",
   "metadata": {},
   "source": [
    "# Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89426ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_indices(points: np.ndarray,\n",
    "                      n_slices: int = 40,\n",
    "                      slice_thickness: float = 0.05\n",
    "                      ) -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Fixed-width slicing along the PCA axis with diagnostics.\"\"\"\n",
    "    if points.ndim != 2 or points.shape[1] != 3:\n",
    "        raise ValueError(f\"Expected points to have shape (N, 3); got {points.shape}\")\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    t = pca.fit_transform(points).ravel()\n",
    "\n",
    "    centers = np.linspace(t.min(), t.max(), n_slices)\n",
    "\n",
    "    per_slice_counts: List[int] = []\n",
    "    selected = set()\n",
    "    half_thickness = slice_thickness / 2.0\n",
    "\n",
    "    for center in centers:\n",
    "        mask = np.abs(t - center) <= half_thickness\n",
    "        hits = np.where(mask)[0]\n",
    "        per_slice_counts.append(int(hits.size))\n",
    "        selected.update(hits.tolist())\n",
    "\n",
    "    indices = np.array(sorted(selected), dtype=int)\n",
    "    total_selected = len(indices)\n",
    "    avg_per_slice = float(np.mean(per_slice_counts)) if per_slice_counts else 0.0\n",
    "\n",
    "    print(f\"[get_slice_indices] total nuclei selected: {total_selected}\")\n",
    "    print(f\"[get_slice_indices] per-slice counts: {per_slice_counts}\")\n",
    "    print(f\"[get_slice_indices] average nuclei per slice: {avg_per_slice:.2f}\")\n",
    "\n",
    "    return indices, per_slice_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SLICES = 40\n",
    "SLICE_THICKNESS = 0.005  # units must match the input coordinates\n",
    "specimens_per_file = 2**14\n",
    "\n",
    "sliced_samples: List[np.ndarray] = []\n",
    "\n",
    "for canonical_ids, coords in all_worms:\n",
    "    coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "    ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "    if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "        raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "\n",
    "    selected_idx, per_slice_counts = get_slice_indices(\n",
    "        coords_arr,\n",
    "        n_slices=NUM_SLICES,\n",
    "        slice_thickness=SLICE_THICKNESS,\n",
    "    )\n",
    "\n",
    "    if selected_idx.size == 0:\n",
    "        continue  # no nuclei captured for this worm\n",
    "\n",
    "    sliced_ids = ids_arr[selected_idx]\n",
    "    sliced_coords = coords_arr[selected_idx]\n",
    "\n",
    "    sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "    sample[:, 0] = sliced_ids.astype(np.float32)\n",
    "    sample[:, 1:] = sliced_coords\n",
    "    sliced_samples.append(sample)\n",
    "\n",
    "    print(\n",
    "        f\"Sliced worm (len={len(ids_arr)}): \"\n",
    "        f\"{len(selected_idx)} nuclei selected | per-slice counts = {per_slice_counts}\"\n",
    "    )\n",
    "\n",
    "# Persist all sliced specimens in one HDF5 shard (adjust output path as needed)\n",
    "output_dir = subgraph_output_directory / \"sliced_subgraphs\"\n",
    "convert_specimens_to_hdf5(\n",
    "    sliced_samples,\n",
    "    output_dir=output_dir,\n",
    "    specimens_per_file=specimens_per_file,\n",
    ")\n",
    "print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfd07d",
   "metadata": {},
   "source": [
    "# Slicing with a shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60dcfa-ae49-4793-bfd0-337ac1d25b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_indices(points: np.ndarray,\n",
    "                      n_slices: int = 40,\n",
    "                      slice_thickness: float = 0.005,\n",
    "                      shift: float = 0.0\n",
    "                      ) -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Fixed-width slicing along the PCA axis with diagnostics.\n",
    "\n",
    "    Args:\n",
    "        points          : [N, 3] array of nucleus centers.\n",
    "        n_slices        : number of slices (default 40).\n",
    "        slice_thickness : axial thickness of each slice (same units as points).\n",
    "        shift           : axial offset applied to every slice center. Positive\n",
    "                          values slide the entire stack toward larger PCA coordinates.\n",
    "\n",
    "    Returns:\n",
    "        indices          : sorted unique nucleus indices captured by at least one slice.\n",
    "        per_slice_counts : hit counts for each slice after the shift.\n",
    "    \"\"\"\n",
    "    if points.ndim != 2 or points.shape[1] != 3:\n",
    "        raise ValueError(f\"Expected points to have shape (N, 3); got {points.shape}\")\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    t = pca.fit_transform(points).ravel()\n",
    "\n",
    "    centers = np.linspace(t.min(), t.max(), n_slices) + shift\n",
    "\n",
    "    per_slice_counts: List[int] = []\n",
    "    selected: set[int] = set()\n",
    "    half_thickness = slice_thickness / 2.0\n",
    "\n",
    "    for center in centers:\n",
    "        mask = np.abs(t - center) <= half_thickness\n",
    "        hits = np.where(mask)[0]\n",
    "        per_slice_counts.append(int(hits.size))\n",
    "        selected.update(hits.tolist())\n",
    "\n",
    "    indices = np.array(sorted(selected), dtype=int)\n",
    "    total_selected = len(indices)\n",
    "    avg_per_slice = float(np.mean(per_slice_counts)) if per_slice_counts else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"[get_slice_indices] shift={shift:.4f} | total nuclei selected: {total_selected} | \"\n",
    "        f\"average per slice: {avg_per_slice:.2f}\"\n",
    "    )\n",
    "\n",
    "    return indices, per_slice_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SLICES = 20  \n",
    "SLICE_THICKNESS = 0.005 # it can be relative to the worm length so as to work with subgraph matching (it can be some ratio of the worm length)\n",
    "NUM_SHIFTS = 24                     # 12 shifts × 200 worms = 2400 samples (match what we had for the random subgraphs for now)\n",
    "                                 \n",
    "center_spacing = 1 / (NUM_SLICES - 1)   # distance between consecutive slice centres. It is fixed to 1 / ... because all wors are scaled to length 1\n",
    "max_non_overlap_shift = max(0.0, center_spacing - SLICE_THICKNESS)\n",
    "\n",
    "SHIFT_STEPS = np.linspace(0.0, max_non_overlap_shift, NUM_SHIFTS)\n",
    "# e.g. [0.0000, 0.0018, 0.0036, …, 0.0200] — still within the original band\n",
    "\n",
    "specimens_per_file = 2**14 # should be changed, carries no meaning\n",
    "\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/sliced_testing_data/20_0007_no_proj\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"NUM_SLICES = {NUM_SLICES}\")\n",
    "print(f\"SLICE_THICKNESS = {SLICE_THICKNESS}\")\n",
    "print(f\"NUM_SHIFTS = {NUM_SHIFTS}\")\n",
    "print(f\"center_spacing = {center_spacing:.6f}\")\n",
    "print(f\"max_non_overlap_shift = {max_non_overlap_shift:.6f}\")\n",
    "print(\"SHIFT_STEPS =\", \", \".join(f\"{s:.6f}\" for s in SHIFT_STEPS))\n",
    "print(f\"specimens_per_file = {specimens_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in SHIFT_STEPS:\n",
    "    sliced_samples: List[np.ndarray] = []\n",
    "    per_worm_selected: List[int] = []\n",
    "    per_worm_slice_counts: List[List[int]] = []\n",
    "\n",
    "    for canonical_ids, coords in all_worms:\n",
    "        coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "        ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "        if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "\n",
    "        selected_idx, per_slice_counts = get_slice_indices(\n",
    "            coords_arr,\n",
    "            n_slices=NUM_SLICES,\n",
    "            slice_thickness=SLICE_THICKNESS,\n",
    "            shift=shift,\n",
    "        )\n",
    "\n",
    "        if selected_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        per_worm_selected.append(len(selected_idx))\n",
    "        per_worm_slice_counts.append(per_slice_counts)\n",
    "\n",
    "        sliced_ids = ids_arr[selected_idx]\n",
    "        sliced_coords = coords_arr[selected_idx]\n",
    "\n",
    "        sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "        sample[:, 0] = sliced_ids.astype(np.float32)\n",
    "        sample[:, 1:] = sliced_coords\n",
    "        sliced_samples.append(sample)\n",
    "\n",
    "        print(\n",
    "            f\"Sliced worm (len={len(ids_arr)}) @ shift {shift:.4f}: \"\n",
    "            f\"{len(selected_idx)} nuclei | per-slice counts = {per_slice_counts}\"\n",
    "        )\n",
    "\n",
    "    if not sliced_samples:\n",
    "        print(f\"No samples produced for shift {shift:.4f}; skipping file output.\")\n",
    "        continue\n",
    "\n",
    "    output_dir = subgraph_output_directory / f\"sliced_subgraphs_shift_{shift:.3f}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        sliced_samples,\n",
    "        output_dir=output_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n",
    "\n",
    "    # ---- summary stats -----------------------------------------------------\n",
    "\n",
    "    avg_selected = float(np.mean(per_worm_selected))\n",
    "    std_selected = float(np.std(per_worm_selected))\n",
    "\n",
    "    slice_matrix = np.asarray(per_worm_slice_counts, dtype=float)\n",
    "    avg_slice_counts = slice_matrix.mean(axis=0)\n",
    "    std_slice_counts = slice_matrix.std(axis=0)\n",
    "    avg_per_slice_overall = float(avg_slice_counts.mean())\n",
    "\n",
    "    stats_path = output_dir / \"slice_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"shift: {shift:.4f}\\n\")\n",
    "        fh.write(f\"num_worms: {len(per_worm_selected)}\\n\")\n",
    "        fh.write(f\"avg_total_nuclei: {avg_selected:.2f}\\n\")\n",
    "        fh.write(f\"std_total_nuclei: {std_selected:.2f}\\n\")\n",
    "        fh.write(f\"avg_per_slice_overall: {avg_per_slice_overall:.2f}\\n\")\n",
    "        fh.write(\"avg_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in avg_slice_counts) + \"\\n\")\n",
    "        fh.write(\"std_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in std_slice_counts) + \"\\n\")\n",
    "    # ------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a268d",
   "metadata": {},
   "source": [
    "# Projecting onto the slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_indices(points: np.ndarray,\n",
    "                      n_slices: int = 40,\n",
    "                      slice_thickness: float = 0.005,\n",
    "                      shift: float = 0.0\n",
    "                      ) -> Tuple[np.ndarray, List[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Select nuclei with fixed-width slices and return their indices plus\n",
    "    the slice-projected coordinates.\n",
    "\n",
    "    Returns:\n",
    "        indices           : sorted unique nucleus indices captured by at least one slice\n",
    "        per_slice_counts  : hit count for each slice (after applying `shift`)\n",
    "        projected_coords  : (len(indices), 3) array containing the coordinates\n",
    "                            projected onto the slice planes, ordered to match `indices`\n",
    "    \"\"\"\n",
    "    if points.ndim != 2 or points.shape[1] != 3:\n",
    "        raise ValueError(f\"Expected points to have shape (N, 3); got {points.shape}\")\n",
    "\n",
    "    pca = PCA(n_components=3)                         # fit PCA to learn the body axis and a full orthonormal basis\n",
    "    pca.fit(points)\n",
    "    axis = pca.components_[0]                        # unit vector along the dominant anatomical axis\n",
    "    t = (points - pca.mean_) @ axis                  # scalar coordinate for each nucleus along that axis\n",
    "\n",
    "    centers = np.linspace(t.min(), t.max(), n_slices) + shift  # evenly spaced slice centers, shifted as requested\n",
    "\n",
    "    per_slice_counts: List[int] = []\n",
    "    selected: Dict[int, Dict[str, Any]] = {}\n",
    "    half_thickness = slice_thickness / 2.0\n",
    "\n",
    "    for center in centers:\n",
    "        mask = np.abs(t - center) <= half_thickness   # nuclei whose axial distance from this slice is within half-thickness\n",
    "        hits = np.where(mask)[0]\n",
    "        per_slice_counts.append(int(hits.size))\n",
    "\n",
    "        for idx in hits:\n",
    "            offset = t[idx] - center                  # signed distance from nucleus center to slice center along the axis\n",
    "            abs_offset = abs(offset)\n",
    "\n",
    "            prev = selected.get(idx)                  # if the nucleus belongs to multiple slices, keep the closest one\n",
    "            if prev is None or abs_offset < prev[\"abs_offset\"]:\n",
    "                projected = points[idx] - offset * axis     # drop the perpendicular component onto the slice plane\n",
    "                selected[idx] = {\n",
    "                    \"abs_offset\": abs_offset,\n",
    "                    \"projected\": projected,\n",
    "                }\n",
    "\n",
    "    if not selected:\n",
    "        return np.array([], dtype=int), per_slice_counts, np.zeros((0, 3), dtype=points.dtype)\n",
    "\n",
    "    indices = np.array(sorted(selected.keys()), dtype=int)\n",
    "    projected_coords = np.stack([selected[idx][\"projected\"] for idx in indices], axis=0)\n",
    "\n",
    "    total_selected = len(indices)\n",
    "    avg_per_slice = float(np.mean(per_slice_counts)) if per_slice_counts else 0.0\n",
    "    print(\n",
    "        f\"[get_slice_indices] shift={shift:.4f} | total nuclei selected: {total_selected} | \"\n",
    "        f\"average per slice: {avg_per_slice:.2f}\"\n",
    "    )\n",
    "\n",
    "    return indices, per_slice_counts, projected_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9077f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SLICES = 40\n",
    "SLICE_THICKNESS = 0.005\n",
    "NUM_SHIFTS = 12                     # 12 shifts × 200 worms = 2400 samples\n",
    "                                 \n",
    "center_spacing = 1 / (NUM_SLICES - 1)   # distance between consecutive slice centres. It is fixed to 1 / ... because all wors are scaled to length 1\n",
    "max_non_overlap_shift = max(0.0, center_spacing - SLICE_THICKNESS)\n",
    "\n",
    "SHIFT_STEPS = np.linspace(0.0, max_non_overlap_shift, NUM_SHIFTS)\n",
    "# e.g. [0.0000, 0.0018, 0.0036, …, 0.0200] — still within the original band\n",
    "\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/sliced_testing_data/20_0007_with_proj\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "specimens_per_file = 2**14 # should be changed, carries no meaning\n",
    "\n",
    "print(f\"NUM_SLICES = {NUM_SLICES}\")\n",
    "print(f\"SLICE_THICKNESS = {SLICE_THICKNESS}\")\n",
    "print(f\"NUM_SHIFTS = {NUM_SHIFTS}\")\n",
    "print(f\"center_spacing = {center_spacing:.6f}\")\n",
    "print(f\"max_non_overlap_shift = {max_non_overlap_shift:.6f}\")\n",
    "print(\"SHIFT_STEPS =\", \", \".join(f\"{s:.6f}\" for s in SHIFT_STEPS))\n",
    "print(f\"specimens_per_file = {specimens_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in SHIFT_STEPS:\n",
    "    sliced_samples: List[np.ndarray] = []\n",
    "    per_worm_selected: List[int] = []\n",
    "    per_worm_slice_counts: List[List[int]] = []\n",
    "\n",
    "    for canonical_ids, coords in all_worms:\n",
    "        coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "        ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "        if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "\n",
    "        selected_idx, per_slice_counts, projected_coords = get_slice_indices(\n",
    "            coords_arr,\n",
    "            n_slices=NUM_SLICES,\n",
    "            slice_thickness=SLICE_THICKNESS,\n",
    "            shift=shift,\n",
    "        )\n",
    "\n",
    "        if selected_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        per_worm_selected.append(len(selected_idx))\n",
    "        per_worm_slice_counts.append(per_slice_counts)\n",
    "\n",
    "        sliced_ids = ids_arr[selected_idx]             # keep canonical IDs in the slice order\n",
    "        sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "        sample[:, 0] = sliced_ids.astype(np.float32)   # column 0: canonical IDs\n",
    "        sample[:, 1:] = projected_coords               # columns 1–3: 3-D coordinates lying on the slice planes\n",
    "        sliced_samples.append(sample)\n",
    "\n",
    "        print(\n",
    "            f\"Sliced worm (len={len(ids_arr)}) @ shift {shift:.4f}: \"\n",
    "            f\"{len(selected_idx)} nuclei | per-slice counts = {per_slice_counts}\"\n",
    "        )\n",
    "\n",
    "    if not sliced_samples:\n",
    "        print(f\"No samples produced for shift {shift:.4f}; skipping file output.\")\n",
    "        continue\n",
    "\n",
    "    output_dir = subgraph_output_directory / f\"sliced_subgraphs_shift_{shift:.3f}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        sliced_samples,\n",
    "        output_dir=output_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n",
    "\n",
    "    avg_selected = float(np.mean(per_worm_selected))\n",
    "    std_selected = float(np.std(per_worm_selected))\n",
    "\n",
    "    slice_matrix = np.asarray(per_worm_slice_counts, dtype=float)\n",
    "    avg_slice_counts = slice_matrix.mean(axis=0)\n",
    "    std_slice_counts = slice_matrix.std(axis=0)\n",
    "    avg_per_slice_overall = float(avg_slice_counts.mean())\n",
    "\n",
    "    stats_path = output_dir / \"slice_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"shift: {shift:.4f}\\n\")\n",
    "        fh.write(f\"num_worms: {len(per_worm_selected)}\\n\")\n",
    "        fh.write(f\"avg_total_nuclei: {avg_selected:.2f}\\n\")\n",
    "        fh.write(f\"std_total_nuclei: {std_selected:.2f}\\n\")\n",
    "        fh.write(f\"avg_per_slice_overall: {avg_per_slice_overall:.2f}\\n\")\n",
    "        fh.write(\"avg_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in avg_slice_counts) + \"\\n\")\n",
    "        fh.write(\"std_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in std_slice_counts) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be3840",
   "metadata": {},
   "source": [
    "# Cross section cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9effcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SLICES = 40\n",
    "SLICE_THICKNESS = 0.005\n",
    "NUM_SHIFTS = 12                     # 12 shifts × 200 worms = 2400 samples\n",
    "                                 \n",
    "center_spacing = 1 / (NUM_SLICES - 1)   # distance between consecutive slice centres. It is fixed to 1 / ... because all wors are scaled to length 1\n",
    "max_non_overlap_shift = max(0.0, center_spacing - SLICE_THICKNESS)\n",
    "\n",
    "SHIFT_STEPS = np.linspace(0.0, max_non_overlap_shift, NUM_SHIFTS)\n",
    "# e.g. [0.0000, 0.0018, 0.0036, …, 0.0200] — still within the original band\n",
    "\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/sliced_tests/sliced_testing_data/cross_section/40_0005_with_proj_y_neg\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "specimens_per_file = 2**14 # should be changed, carries no meaning\n",
    "\n",
    "CROP_AXIS = 'y'\n",
    "CROP_SIDE = 'negative'\n",
    "CROP_FRACTION = 0.0  \n",
    "\n",
    "\n",
    "print(f\"NUM_SLICES = {NUM_SLICES}\")\n",
    "print(f\"SLICE_THICKNESS = {SLICE_THICKNESS}\")\n",
    "print(f\"NUM_SHIFTS = {NUM_SHIFTS}\")\n",
    "print(f\"center_spacing = {center_spacing:.6f}\")\n",
    "print(f\"max_non_overlap_shift = {max_non_overlap_shift:.6f}\")\n",
    "print(\"SHIFT_STEPS =\", \", \".join(f\"{s:.6f}\" for s in SHIFT_STEPS))\n",
    "print(f\"specimens_per_file = {specimens_per_file}\")\n",
    "print(f\"CROP_AXIS = {CROP_AXIS}\")\n",
    "print(f\"CROP_SIDE = {CROP_SIDE}\")\n",
    "print(f\"CROP_FRACTION = {CROP_FRACTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22f040-12b0-4cb4-8968-21ae6dce1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_slice_indices(points: np.ndarray,\n",
    "                      n_slices: int = 40,\n",
    "                      slice_thickness: float = 0.005,\n",
    "                      shift: float = 0.0,\n",
    "                      *,\n",
    "                      crop_axis: Optional[str] = None,\n",
    "                      crop_side: str = \"positive\",\n",
    "                      crop_fraction: float = 0.0\n",
    "                      ) -> Tuple[np.ndarray, List[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Select nuclei with fixed-width slices, project them onto slice planes,\n",
    "    and optionally drop one side of each slice (LR or DV). When a crop is requested,\n",
    "    the entire chosen side is removed with the exception of a thin band (crop_fraction)\n",
    "    near the center line that can remain.\n",
    "\n",
    "    Args:\n",
    "        points          : (N, 3) array of Cartesian nucleus centers.\n",
    "        n_slices        : number of slice planes along the worm’s main axis.\n",
    "        slice_thickness : axial thickness of each slice.\n",
    "        shift           : uniform axial offset applied to every slice center.\n",
    "        crop_axis       : 'x', 'y', or None; choose LR or DV for trimming.\n",
    "        crop_side       : 'positive' or 'negative'; which half-space to mostly remove.\n",
    "        crop_fraction   : fraction (0–1) of the removed side’s span to keep near the center.\n",
    "\n",
    "    Returns:\n",
    "        indices           : sorted nucleus indices retained after slicing/cropping.\n",
    "        per_slice_counts  : nuclei per slice after all filters.\n",
    "        projected_coords  : coordinates projected onto their slice planes.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=3)               # Fit PCA to get axial & slice-plane bases\n",
    "    pca.fit(points)\n",
    "\n",
    "    axial_axis = pca.components_[0]         # dominant head–tail direction\n",
    "    mean = pca.mean_\n",
    "    t = (points - mean) @ axial_axis        # axial coordinate for each nucleus\n",
    "\n",
    "    global_x = np.array([1.0, 0.0, 0.0])    # dataset LR direction\n",
    "    global_y = np.array([0.0, 1.0, 0.0])    # dataset DV direction\n",
    "    in_plane = [pca.components_[1], pca.components_[2]]  # orthonormal slice-plane basis\n",
    "\n",
    "    dot_lr = [abs(np.dot(vec, global_x)) for vec in in_plane]\n",
    "    lr_index = int(np.argmax(dot_lr))       # pick the basis vector that aligns best with +x\n",
    "    plane_lr = in_plane[lr_index].copy()\n",
    "    plane_dv = in_plane[1 - lr_index].copy()\n",
    "\n",
    "    if np.dot(plane_lr, global_x) < 0:      # ensure +LR points to +x\n",
    "        plane_lr *= -1\n",
    "    if np.dot(plane_dv, global_y) < 0:      # ensure +DV points to +y\n",
    "        plane_dv *= -1\n",
    "\n",
    "    centered = points - mean                # center cloud before in-plane projection\n",
    "    local_lr = centered @ plane_lr          # LR coordinate per nucleus\n",
    "    local_dv = centered @ plane_dv          # DV coordinate per nucleus\n",
    "\n",
    "    centers = np.linspace(t.min(), t.max(), n_slices) + shift  # slice centers along axis\n",
    "    selected: Dict[int, Dict[str, Any]] = {}\n",
    "    half_thickness = slice_thickness / 2.0\n",
    "\n",
    "    for slice_idx, center in enumerate(centers):\n",
    "        mask = np.abs(t - center) <= half_thickness\n",
    "        hits = np.where(mask)[0]\n",
    "\n",
    "        if crop_axis in {\"x\", \"y\"} and hits.size > 0:\n",
    "            slice_values = local_lr[hits] if crop_axis == \"x\" else local_dv[hits]\n",
    "\n",
    "            if crop_side == \"positive\":\n",
    "                # keep everything at or below the center line (value <= 0)\n",
    "                keep_mask = slice_values <= 0.0\n",
    "                if crop_fraction > 0.0:\n",
    "                    pos_vals = slice_values[slice_values > 0.0]\n",
    "                    if pos_vals.size > 0:\n",
    "                        span = pos_vals.max()          # farthest reach on the removed side\n",
    "                        allowed = span * crop_fraction # band to keep near the center\n",
    "                        keep_mask |= (slice_values > 0.0) & (slice_values <= allowed)\n",
    "            else:\n",
    "                keep_mask = slice_values >= 0.0\n",
    "                if crop_fraction > 0.0:\n",
    "                    neg_vals = slice_values[slice_values < 0.0]\n",
    "                    if neg_vals.size > 0:\n",
    "                        span = abs(neg_vals.min())\n",
    "                        allowed = span * crop_fraction\n",
    "                        keep_mask |= (slice_values < 0.0) & (slice_values >= -allowed)\n",
    "\n",
    "            hits = hits[keep_mask]\n",
    "\n",
    "        for idx in hits:\n",
    "            offset = t[idx] - center\n",
    "            abs_offset = abs(offset)\n",
    "\n",
    "            prev = selected.get(idx)\n",
    "            if prev is None or abs_offset < prev[\"abs_offset\"]:\n",
    "                projected = points[idx] - offset * axial_axis  # drop axial component → slice plane\n",
    "                selected[idx] = {\n",
    "                    \"abs_offset\": abs_offset,\n",
    "                    \"projected\": projected,\n",
    "                    \"slice_index\": slice_idx,\n",
    "                }\n",
    "\n",
    "    if not selected:\n",
    "        empty_counts = [0] * n_slices\n",
    "        empty_coords = np.zeros((0, 3), dtype=points.dtype)\n",
    "        return np.array([], dtype=int), empty_counts, empty_coords\n",
    "\n",
    "    per_slice_counts = [0] * n_slices\n",
    "    for entry in selected.values():\n",
    "        per_slice_counts[entry[\"slice_index\"]] += 1\n",
    "\n",
    "    indices = np.array(sorted(selected.keys()), dtype=int)\n",
    "    projected_coords = np.stack([selected[idx][\"projected\"] for idx in indices], axis=0)\n",
    "\n",
    "    total_selected = len(indices)\n",
    "    avg_per_slice = float(np.mean(per_slice_counts)) if per_slice_counts else 0.0\n",
    "    print(\n",
    "        f\"[get_slice_indices] shift={shift:.4f} | total nuclei selected: {total_selected} | \"\n",
    "        f\"average per slice: {avg_per_slice:.2f}\"\n",
    "    )\n",
    "\n",
    "    return indices, per_slice_counts, projected_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in SHIFT_STEPS:\n",
    "    sliced_samples: List[np.ndarray] = []\n",
    "    per_worm_selected: List[int] = []\n",
    "    per_worm_slice_counts: List[List[int]] = []\n",
    "\n",
    "    for canonical_ids, coords in all_worms:\n",
    "        coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "        ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "        if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "\n",
    "        selected_idx, per_slice_counts, projected_coords = get_slice_indices(\n",
    "            coords_arr,\n",
    "            n_slices=NUM_SLICES,\n",
    "            slice_thickness=SLICE_THICKNESS,\n",
    "            shift=shift,\n",
    "            crop_axis=CROP_AXIS,\n",
    "            crop_side=CROP_SIDE,\n",
    "            crop_fraction=CROP_FRACTION,\n",
    "        )\n",
    "\n",
    "        if selected_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        per_worm_selected.append(len(selected_idx))\n",
    "        per_worm_slice_counts.append(per_slice_counts)\n",
    "\n",
    "        sliced_ids = ids_arr[selected_idx]             # keep canonical IDs in the slice order\n",
    "        sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "        sample[:, 0] = sliced_ids.astype(np.float32)   # column 0: canonical IDs\n",
    "        sample[:, 1:] = projected_coords               # columns 1–3: 3-D coordinates lying on the slice planes\n",
    "        sliced_samples.append(sample)\n",
    "\n",
    "        print(\n",
    "            f\"Sliced worm (len={len(ids_arr)}) @ shift {shift:.4f}: \"\n",
    "            f\"{len(selected_idx)} nuclei | per-slice counts = {per_slice_counts}\"\n",
    "        )\n",
    "\n",
    "    if not sliced_samples:\n",
    "        print(f\"No samples produced for shift {shift:.4f}; skipping file output.\")\n",
    "        continue\n",
    "\n",
    "    output_dir = subgraph_output_directory / f\"sliced_subgraphs_shift_{shift:.3f}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        sliced_samples,\n",
    "        output_dir=output_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n",
    "\n",
    "    avg_selected = float(np.mean(per_worm_selected))\n",
    "    std_selected = float(np.std(per_worm_selected))\n",
    "\n",
    "    slice_matrix = np.asarray(per_worm_slice_counts, dtype=float)\n",
    "    avg_slice_counts = slice_matrix.mean(axis=0)\n",
    "    std_slice_counts = slice_matrix.std(axis=0)\n",
    "    avg_per_slice_overall = float(avg_slice_counts.mean())\n",
    "\n",
    "    stats_path = output_dir / \"slice_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"shift: {shift:.4f}\\n\")\n",
    "        fh.write(f\"num_worms: {len(per_worm_selected)}\\n\")\n",
    "        fh.write(f\"avg_total_nuclei: {avg_selected:.2f}\\n\")\n",
    "        fh.write(f\"std_total_nuclei: {std_selected:.2f}\\n\")\n",
    "        fh.write(f\"avg_per_slice_overall: {avg_per_slice_overall:.2f}\\n\")\n",
    "        fh.write(\"avg_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in avg_slice_counts) + \"\\n\")\n",
    "        fh.write(\"std_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in std_slice_counts) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147124d4",
   "metadata": {},
   "source": [
    "# Alignment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f695cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def check_alignment_all_with_ids(\n",
    "    worms_iterable,\n",
    "    *,\n",
    "    z_axis=np.array([0.0, 0.0, 1.0]),\n",
    "    x_axis=np.array([1.0, 0.0, 0.0]),\n",
    "    y_axis=np.array([0.0, 1.0, 0.0]),\n",
    "    good_thresh: float = 0.98,     # |cos(AP,Z)| >= 0.98 ≈ tilt <= ~11.5°\n",
    "    warn_thresh: float = 0.95      # |cos(AP,Z)| >= 0.95 ≈ tilt <= ~18.2°\n",
    "):\n",
    "    \"\"\"\n",
    "    worms_iterable: iterable of (canonical_ids, coords)\n",
    "        canonical_ids: (N,) int-like or str-like IDs\n",
    "        coords:        (N, 3) float array with (x=LR, y=DV, z=AP) if aligned\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"rows\": list of per-worm dicts,\n",
    "          \"cos_ap_z\": np.ndarray,\n",
    "          \"summary\": dict\n",
    "        }\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cos_vals = []\n",
    "\n",
    "    for w_idx, (canonical_ids, coords) in enumerate(worms_iterable):\n",
    "        coords = np.asarray(coords, dtype=float)\n",
    "        if coords.ndim != 2 or coords.shape[1] != 3:\n",
    "            raise ValueError(f\"Worm {w_idx}: coords must be (N,3), got {coords.shape}\")\n",
    "\n",
    "        # PCA → AP axis\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(coords)\n",
    "        ap_axis = pca.components_[0]\n",
    "        ap_axis = ap_axis / np.linalg.norm(ap_axis)\n",
    "\n",
    "        # Cosines with global axes\n",
    "        cZ = float(np.dot(ap_axis, z_axis))\n",
    "        cY = float(np.dot(ap_axis, y_axis))\n",
    "        cX = float(np.dot(ap_axis, x_axis))\n",
    "\n",
    "        # Tilt angle from Z in degrees (use absolute to ignore head/tail sign)\n",
    "        tilt_deg = float(np.degrees(np.arccos(min(1.0, max(-1.0, abs(cZ))))))\n",
    "\n",
    "        # Keep a small ID fingerprint for reference (first few canonical IDs)\n",
    "        ids_arr = np.asarray(canonical_ids)\n",
    "        id_preview = ids_arr[:3].tolist() if ids_arr.ndim == 1 else None\n",
    "\n",
    "        row = {\n",
    "            \"worm_index\": w_idx,\n",
    "            \"n_points\": int(coords.shape[0]),\n",
    "            \"cos_ap_z\": cZ,\n",
    "            \"cos_ap_y\": cY,\n",
    "            \"cos_ap_x\": cX,\n",
    "            \"tilt_deg_from_z\": tilt_deg,\n",
    "            \"id_preview\": id_preview,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        cos_vals.append(cZ)\n",
    "\n",
    "        print(\n",
    "            f\"Worm {w_idx:03d} | N={coords.shape[0]:4d} | \"\n",
    "            f\"AP•Z={cZ:+.4f} | tilt={tilt_deg:6.2f}° | \"\n",
    "            f\"AP•X={cX:+.4f} AP•Y={cY:+.4f} | ids={id_preview}\"\n",
    "        )\n",
    "\n",
    "    cos_vals = np.asarray(cos_vals, dtype=float)\n",
    "    abs_cos = np.abs(cos_vals)\n",
    "\n",
    "    # Summary stats\n",
    "    mean_cos = float(np.mean(cos_vals)) if cos_vals.size else float(\"nan\")\n",
    "    std_cos  = float(np.std(cos_vals))  if cos_vals.size else float(\"nan\")\n",
    "    min_cos  = float(np.min(cos_vals))  if cos_vals.size else float(\"nan\")\n",
    "    max_cos  = float(np.max(cos_vals))  if cos_vals.size else float(\"nan\")\n",
    "\n",
    "    pct_good = float(np.mean(abs_cos >= good_thresh) * 100.0) if cos_vals.size else 0.0\n",
    "    pct_warn = float(np.mean((abs_cos >= warn_thresh) & (abs_cos < good_thresh)) * 100.0) if cos_vals.size else 0.0\n",
    "    pct_bad  = 100.0 - pct_good - pct_warn if cos_vals.size else 0.0\n",
    "\n",
    "    print(\"\\nSummary over worms\")\n",
    "    print(f\"Mean cos(AP,Z): {mean_cos:+.4f} | Std: {std_cos:.4f} | Min: {min_cos:+.4f} | Max: {max_cos:+.4f}\")\n",
    "    print(f\"Well aligned   (|cos| ≥ {good_thresh:.2f}): {pct_good:5.1f}%\")\n",
    "    print(f\"Moderate tilt  (|cos| ≥ {warn_thresh:.2f} & < {good_thresh:.2f}): {pct_warn:5.1f}%\")\n",
    "    print(f\"Poor alignment (|cos| <  {warn_thresh:.2f}): {pct_bad:5.1f}%\")\n",
    "\n",
    "    summary = {\n",
    "        \"mean_cos_ap_z\": mean_cos,\n",
    "        \"std_cos_ap_z\": std_cos,\n",
    "        \"min_cos_ap_z\": min_cos,\n",
    "        \"max_cos_ap_z\": max_cos,\n",
    "        \"pct_well_aligned\": pct_good,\n",
    "        \"pct_moderate_tilt\": pct_warn,\n",
    "        \"pct_poor_alignment\": pct_bad,\n",
    "        \"good_thresh\": good_thresh,\n",
    "        \"warn_thresh\": warn_thresh,\n",
    "    }\n",
    "\n",
    "    return {\"rows\": rows, \"cos_ap_z\": cos_vals, \"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def check_alignment_xy_and_plane_with_ids(\n",
    "    worms_iterable,\n",
    "    *,\n",
    "    z_axis=np.array([0.0, 0.0, 1.0]),\n",
    "    x_axis=np.array([1.0, 0.0, 0.0]),\n",
    "    y_axis=np.array([0.0, 1.0, 0.0]),\n",
    "    ap_good_thresh: float = 0.98   # |cos(AP,Z)| >= 0.98 considered well-aligned\n",
    "):\n",
    "    \"\"\"\n",
    "    Extended alignment diagnostics per worm.\n",
    "\n",
    "    Input:\n",
    "      worms_iterable: iterable of (canonical_ids, coords) with coords (N,3)\n",
    "\n",
    "    Prints (per worm):\n",
    "      - AP·Z and tilt angle from Z\n",
    "      - ap·x and ap·y (should be ~0 if AP ≡ Z)\n",
    "      - |PC1·x|, |PC1·y|, |PC2·x|, |PC2·y| (in-plane relationships)\n",
    "      - handedness sign of [PC1, PC2, AP] (should be +1 for right-handed)\n",
    "\n",
    "    Returns:\n",
    "      dict with per-worm rows and simple summary on AP·Z.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cos_ap_z_all = []\n",
    "\n",
    "    for w_idx, (canonical_ids, coords) in enumerate(worms_iterable):\n",
    "        P = np.asarray(coords, dtype=float)\n",
    "        if P.ndim != 2 or P.shape[1] != 3:\n",
    "            raise ValueError(f\"Worm {w_idx}: coords must be (N,3), got {P.shape}\")\n",
    "\n",
    "        pca = PCA(n_components=3).fit(P)\n",
    "        # By sklearn convention, components_[0] has the largest variance (AP axis)\n",
    "        ap = pca.components_[0]; ap /= np.linalg.norm(ap)\n",
    "        pc1 = pca.components_[1]; pc1 /= np.linalg.norm(pc1)\n",
    "        pc2 = pca.components_[2]; pc2 /= np.linalg.norm(pc2)\n",
    "\n",
    "        # Make sure pc2 is orthogonal to ap and pc1 numerically (for sanity)\n",
    "        # (PCA already gives orthonormal vectors; we just rely on sklearn.)\n",
    "        cos_ap_z = float(np.dot(ap, z_axis))\n",
    "        tilt_deg = float(np.degrees(np.arccos(np.clip(abs(cos_ap_z), -1.0, 1.0))))\n",
    "\n",
    "        # Orthogonality of global x,y with AP (should be ~0 if AP ≡ Z)\n",
    "        ap_dot_x = float(np.dot(ap, x_axis))\n",
    "        ap_dot_y = float(np.dot(ap, y_axis))\n",
    "\n",
    "        # In-plane relationships (absolute cosines)\n",
    "        c_pc1_x = float(abs(np.dot(pc1, x_axis)))\n",
    "        c_pc1_y = float(abs(np.dot(pc1, y_axis)))\n",
    "        c_pc2_x = float(abs(np.dot(pc2, x_axis)))\n",
    "        c_pc2_y = float(abs(np.dot(pc2, y_axis)))\n",
    "\n",
    "        # Handedness of the PCA basis\n",
    "        handed = np.linalg.det(np.stack([pc1, pc2, ap], axis=1))\n",
    "        handed_sign = +1 if handed > 0 else -1\n",
    "\n",
    "        rows.append({\n",
    "            \"worm_index\": w_idx,\n",
    "            \"n_points\": int(P.shape[0]),\n",
    "            \"cos_ap_z\": cos_ap_z,\n",
    "            \"tilt_deg_from_z\": tilt_deg,\n",
    "            \"ap_dot_x\": ap_dot_x,\n",
    "            \"ap_dot_y\": ap_dot_y,\n",
    "            \"abs_pc1_dot_x\": c_pc1_x,\n",
    "            \"abs_pc1_dot_y\": c_pc1_y,\n",
    "            \"abs_pc2_dot_x\": c_pc2_x,\n",
    "            \"abs_pc2_dot_y\": c_pc2_y,\n",
    "            \"handed_sign\": handed_sign,\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Worm {w_idx:03d} | N={P.shape[0]:4d} | \"\n",
    "            f\"AP•Z={cos_ap_z:+.4f} (tilt={tilt_deg:5.2f}°) | \"\n",
    "            f\"AP•X={ap_dot_x:+.4e} AP•Y={ap_dot_y:+.4e} | \"\n",
    "            f\"|PC1•X|={c_pc1_x:.3f} |PC1•Y|={c_pc1_y:.3f} | \"\n",
    "            f\"|PC2•X|={c_pc2_x:.3f} |PC2•Y|={c_pc2_y:.3f} | \"\n",
    "            f\"handed={'RH' if handed_sign>0 else 'LH'}\"\n",
    "        )\n",
    "\n",
    "        cos_ap_z_all.append(cos_ap_z)\n",
    "\n",
    "    cos_ap_z_all = np.asarray(cos_ap_z_all, dtype=float)\n",
    "    pct_aligned = float(np.mean(np.abs(cos_ap_z_all) >= ap_good_thresh) * 100.0) if cos_ap_z_all.size else 0.0\n",
    "    print(f\"\\nWell aligned to Z (|AP•Z| >= {ap_good_thresh:.2f}): {pct_aligned:.1f}%\")\n",
    "\n",
    "    return {\n",
    "        \"rows\": rows,\n",
    "        \"cos_ap_z\": cos_ap_z_all,\n",
    "        \"pct_well_aligned\": pct_aligned,\n",
    "        \"ap_good_thresh\": ap_good_thresh,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = check_alignment_xy_and_plane_with_ids(all_worms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982e9c8",
   "metadata": {},
   "source": [
    "# New cross section cropping slicing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SLICES = 20\n",
    "SLICE_THICKNESS = 0.008\n",
    "NUM_SHIFTS = 12                     # 12 shifts × 200 worms = 2400 samples\n",
    "                                 \n",
    "center_spacing = 1 / (NUM_SLICES - 1)   # distance between consecutive slice centres. It is fixed to 1 / ... because all worms are scaled to length 1\n",
    "max_non_overlap_shift = max(0.0, center_spacing - SLICE_THICKNESS)\n",
    "\n",
    "SHIFT_STEPS = np.linspace(0.0, max_non_overlap_shift, NUM_SHIFTS)\n",
    "# e.g. [0.0000, 0.0018, 0.0036, …, 0.0200] — still within the original band\n",
    "\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/new_data/20_0008_05\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "specimens_per_file = 2**14 # should be changed, carries no meaning\n",
    "\n",
    "CROP_AXIS = 'random'\n",
    "CROP_FRACTION_RANGE = (0.0, np.nextafter(1.0, np.inf))\n",
    "\n",
    "seed = 42\n",
    "RNG = np.random.default_rng(seed)\n",
    "\n",
    "# CROP_SIDE = 'positive'\n",
    "# CROP_FRACTION = 0.25\n",
    "\n",
    "print(f\"NUM_SLICES = {NUM_SLICES}\")\n",
    "print(f\"SLICE_THICKNESS = {SLICE_THICKNESS}\")\n",
    "print(f\"NUM_SHIFTS = {NUM_SHIFTS}\")\n",
    "print(f\"center_spacing = {center_spacing:.6f}\")\n",
    "print(f\"max_non_overlap_shift = {max_non_overlap_shift:.6f}\")\n",
    "print(\"SHIFT_STEPS =\", \", \".join(f\"{s:.6f}\" for s in SHIFT_STEPS))\n",
    "print(f\"specimens_per_file = {specimens_per_file}\")\n",
    "print(f\"CROP_AXIS = {CROP_AXIS}\")\n",
    "# print(f\"CROP_SIDE = {CROP_SIDE}\")\n",
    "print(f\"CROP_FRACTION_RANGE = {CROP_FRACTION_RANGE}\")\n",
    "print(f\"RNG seed = {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_slice_indices(\n",
    "    points: np.ndarray,\n",
    "    n_slices: int = 40,\n",
    "    slice_thickness: float = 0.005,\n",
    "    shift: float = 0.0,\n",
    "    *,\n",
    "    crop_axis: Optional[str] = None,   # 'x', 'y', 'random', or None\n",
    "    crop_side: str = \"positive\",\n",
    "    crop_fraction: float = 0.0,\n",
    "    random_state: Optional[np.random.Generator] = None,\n",
    ") -> Tuple[np.ndarray, List[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Slice a worm‑shaped point cloud into thin slabs and optionally remove one half\n",
    "    of each slice. The main (AP) axis is estimated via PCA; crop_axis controls\n",
    "    whether you drop the positive or negative side of x (LR), y (DV), a random\n",
    "    in‑plane direction, or nothing at all. crop_fraction retains a thin band\n",
    "    near the centreline on the removed side.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : (N, 3) array\n",
    "        Cartesian nucleus centres.  Axes are assumed to be (x=LR, y=DV, z=AP).\n",
    "    n_slices : int\n",
    "        Number of slices along the worm’s length.\n",
    "    slice_thickness : float\n",
    "        Thickness of each axial slab (distance along the AP axis).\n",
    "    shift : float\n",
    "        Uniform shift applied to all slice centres along the AP axis.\n",
    "    crop_axis : {\"x\", \"y\", \"random\", None}\n",
    "        Direction along which to crop within each slice:\n",
    "          • None: no cropping.\n",
    "          • 'x' : remove LR half (positive or negative).\n",
    "          • 'y' : remove DV half.\n",
    "          • 'random': remove half along a random direction in the xy‑plane.\n",
    "    crop_side : {\"positive\", \"negative\"}\n",
    "        Which half to remove (“positive” removes values > 0 and keeps ≤ 0).\n",
    "    crop_fraction : float\n",
    "        Fraction of the removed side’s span to retain near zero. 0.0 keeps\n",
    "        nothing; 1.0 keeps the entire removed side.\n",
    "    random_state : np.random.Generator, optional\n",
    "        Source of randomness for 'random' crop_axis; use to make behaviour reproducible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    indices : (M,) array of int\n",
    "        Sorted indices of nuclei that survived slicing and cropping.\n",
    "    per_slice_counts : list of int\n",
    "        Number of nuclei kept in each of the n_slices axial slabs.\n",
    "    projected_coords : (M, 3) array\n",
    "        The 3D coordinates projected onto their slice planes.\n",
    "    \"\"\"\n",
    "    # Step 1: fit PCA to find the AP axis; use only PC0 for slicing.\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(points)\n",
    "    axial_axis = pca.components_[0]           # unit vector for AP\n",
    "    mean = pca.mean_\n",
    "    t = (points - mean) @ axial_axis          # scalar position along AP for each point\n",
    "\n",
    "    # Centre the cloud once; used for all in‑plane coordinate calculations.\n",
    "    centered = points - mean\n",
    "    slice_coord: Optional[np.ndarray] = None  # will hold signed distances for cropping\n",
    "\n",
    "    # Step 2: choose in‑plane direction based on crop_axis.\n",
    "    if crop_axis is None:\n",
    "        # no cropping requested\n",
    "        pass\n",
    "    elif crop_axis == \"x\":\n",
    "        # crop along LR; positive x values lie on one side, negative on the other\n",
    "        slice_coord = centered[:, 0]\n",
    "    elif crop_axis == \"y\":\n",
    "        # crop along DV\n",
    "        slice_coord = centered[:, 1]\n",
    "    elif crop_axis == \"random\":\n",
    "        # sample a random unit vector in the xy‑plane (perpendicular to z)\n",
    "        rng = random_state or np.random.default_rng()\n",
    "        angle = rng.uniform(0.0, 2.0 * np.pi)\n",
    "        cos_orient = np.cos(angle)\n",
    "        sin_orient = np.sin(angle)\n",
    "        global_x_unit = np.array([1.0, 0.0, 0.0], dtype=points.dtype)\n",
    "        global_y_unit = np.array([0.0, 1.0, 0.0], dtype=points.dtype)\n",
    "        rand_vec = cos_orient * global_x_unit + sin_orient * global_y_unit\n",
    "        slice_coord = centered @ rand_vec\n",
    "    else:\n",
    "        raise ValueError(f\"crop_axis must be 'x', 'y', 'random' or None; got {crop_axis}\")\n",
    "\n",
    "    # Step 3: create slice centres along AP and assign points to slabs.\n",
    "    centres = np.linspace(t.min(), t.max(), n_slices) + shift\n",
    "    selected: Dict[int, Dict[str, Any]] = {}\n",
    "    half_thickness = slice_thickness / 2.0\n",
    "\n",
    "    for slice_idx, centre_val in enumerate(centres):\n",
    "        mask = np.abs(t - centre_val) <= half_thickness\n",
    "        hits = np.where(mask)[0]\n",
    "\n",
    "        # Apply optional in‑plane cropping.\n",
    "        if slice_coord is not None and hits.size > 0:\n",
    "            vals = slice_coord[hits]\n",
    "            if crop_side == \"positive\":\n",
    "                keep_mask = vals <= 0.0\n",
    "                if crop_fraction > 0.0:\n",
    "                    pos_vals = vals[vals > 0.0]\n",
    "                    if pos_vals.size > 0:\n",
    "                        span = pos_vals.max()\n",
    "                        allowed = span * crop_fraction\n",
    "                        keep_mask |= (vals > 0.0) & (vals <= allowed)\n",
    "            else:\n",
    "                keep_mask = vals >= 0.0\n",
    "                if crop_fraction > 0.0:\n",
    "                    neg_vals = vals[vals < 0.0]\n",
    "                    if neg_vals.size > 0:\n",
    "                        span = abs(neg_vals.min())\n",
    "                        allowed = span * crop_fraction\n",
    "                        keep_mask |= (vals < 0.0) & (vals >= -allowed)\n",
    "            hits = hits[keep_mask]\n",
    "\n",
    "        # Project survivors onto their slice plane; deduplicate by keeping the nearest slice.\n",
    "        for idx in hits:\n",
    "            offset = t[idx] - centre_val\n",
    "            abs_offset = abs(offset)\n",
    "            prev = selected.get(idx)\n",
    "            if prev is None or abs_offset < prev[\"abs_offset\"]:\n",
    "                projected = points[idx] - offset * axial_axis\n",
    "                selected[idx] = {\n",
    "                    \"abs_offset\": abs_offset,\n",
    "                    \"projected\": projected,\n",
    "                    \"slice_index\": slice_idx,\n",
    "                }\n",
    "\n",
    "    if not selected:\n",
    "        return np.array([], dtype=int), [0] * n_slices, np.zeros((0, 3), dtype=points.dtype)\n",
    "\n",
    "    per_slice_counts: List[int] = [0] * n_slices\n",
    "    for v in selected.values():\n",
    "        per_slice_counts[v[\"slice_index\"]] += 1\n",
    "\n",
    "    indices = np.array(sorted(selected.keys()), dtype=int)\n",
    "    projected_coords = np.stack([selected[i][\"projected\"] for i in indices], axis=0)\n",
    "\n",
    "    return indices, per_slice_counts, projected_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in SHIFT_STEPS:\n",
    "    sliced_samples: List[np.ndarray] = []\n",
    "    per_worm_selected: List[int] = []\n",
    "    per_worm_slice_counts: List[List[int]] = []\n",
    "\n",
    "    for canonical_ids, coords in all_worms:\n",
    "        coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "        ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "        if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "        \n",
    "        crop_side = RNG.choice((\"positive\", \"negative\"))\n",
    "        crop_fraction = RNG.uniform(*CROP_FRACTION_RANGE)\n",
    "        random_state = np.random.default_rng(RNG.integers(0, 2**32))\n",
    "\n",
    "        selected_idx, per_slice_counts, projected_coords = get_slice_indices(\n",
    "            coords_arr,\n",
    "            n_slices=NUM_SLICES,\n",
    "            slice_thickness=SLICE_THICKNESS,\n",
    "            shift=shift,\n",
    "            crop_axis=CROP_AXIS,\n",
    "            crop_side=crop_side,\n",
    "            crop_fraction=crop_fraction,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        if selected_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        per_worm_selected.append(len(selected_idx))\n",
    "        per_worm_slice_counts.append(per_slice_counts)\n",
    "\n",
    "        sliced_ids = ids_arr[selected_idx]             # keep canonical IDs in the slice order\n",
    "        sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "        sample[:, 0] = sliced_ids.astype(np.float32)   # column 0: canonical IDs\n",
    "        sample[:, 1:] = projected_coords               # columns 1–3: 3-D coordinates lying on the slice planes\n",
    "        sliced_samples.append(sample)\n",
    "\n",
    "        print(\n",
    "            f\"Sliced worm (len={len(ids_arr)}) @ shift {shift:.4f}: \"\n",
    "            f\"{len(selected_idx)} nuclei | per-slice counts = {per_slice_counts}\"\n",
    "            f\"| crop_side={crop_side} | crop_fraction={crop_fraction:.3f}\"\n",
    "        )\n",
    "\n",
    "    if not sliced_samples:\n",
    "        print(f\"No samples produced for shift {shift:.4f}; skipping file output.\")\n",
    "        continue\n",
    "\n",
    "    output_dir = subgraph_output_directory / f\"sliced_subgraphs_shift_{shift:.3f}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        sliced_samples,\n",
    "        output_dir=output_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n",
    "\n",
    "    avg_selected = float(np.mean(per_worm_selected))\n",
    "    std_selected = float(np.std(per_worm_selected))\n",
    "\n",
    "    slice_matrix = np.asarray(per_worm_slice_counts, dtype=float)\n",
    "    avg_slice_counts = slice_matrix.mean(axis=0)\n",
    "    std_slice_counts = slice_matrix.std(axis=0)\n",
    "    avg_per_slice_overall = float(avg_slice_counts.mean())\n",
    "\n",
    "    stats_path = output_dir / \"slice_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"shift: {shift:.4f}\\n\")\n",
    "        fh.write(f\"num_worms: {len(per_worm_selected)}\\n\")\n",
    "        fh.write(f\"avg_total_nuclei: {avg_selected:.2f}\\n\")\n",
    "        fh.write(f\"std_total_nuclei: {std_selected:.2f}\\n\")\n",
    "        fh.write(f\"avg_per_slice_overall: {avg_per_slice_overall:.2f}\\n\")\n",
    "        fh.write(\"avg_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in avg_slice_counts) + \"\\n\")\n",
    "        fh.write(\"std_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in std_slice_counts) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5422be",
   "metadata": {},
   "source": [
    "# Saving the whole worm as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35451c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_specimens_to_hdf5(\n",
    "    specimens_data: Union[np.ndarray, Sequence[Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]]],\n",
    "    output_dir: Union[str, Path],\n",
    "    specimens_per_file: int = 2**14,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save sliced subgraphs (and optional full-worm copies) into test/*.h5 files.\n",
    "\n",
    "    Each element of `specimens_data` may be:\n",
    "      • a single array shaped [M, 4]          → only a subgraph (legacy behaviour)\n",
    "      • a tuple (subgraph, full_worm), both [*, 4] arrays with canonical_id,x,y,z.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    specimens_list = list(specimens_data)\n",
    "    split_dir = output_dir / \"test\"\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    file_idx = 0\n",
    "    specimen_idx = 0\n",
    "\n",
    "    for start_idx in range(0, len(specimens_list), specimens_per_file):\n",
    "        end_idx = min(start_idx + specimens_per_file, len(specimens_list))\n",
    "        batch_data = specimens_list[start_idx:end_idx]\n",
    "\n",
    "        file_path = split_dir / f\"test_{file_idx:04d}.h5\"\n",
    "        specimen_idx = create_hdf5_file_with_full_worms(\n",
    "            batch_data,\n",
    "            file_path,\n",
    "            file_prefix=\"specimen\",\n",
    "            start_idx=specimen_idx,\n",
    "        )\n",
    "        file_idx += 1\n",
    "\n",
    "    info = {\n",
    "        \"total_specimens\": len(specimens_list),\n",
    "        \"split\": \"test\",\n",
    "        \"specimens_per_file\": specimens_per_file,\n",
    "        \"format\": {\n",
    "            \"subgraph\": \"[canonical_id, x, y, z]\",\n",
    "            \"full_worm\": \"[canonical_id, x, y, z]\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(output_dir / \"dataset_info.json\", \"w\") as f:\n",
    "        json.dump(info, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a73f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5_file_with_full_worms(\n",
    "    specimens: Sequence[Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]],\n",
    "    file_path: Union[str, Path],\n",
    "    *,\n",
    "    file_prefix: str = \"specimen\",\n",
    "    start_idx: int = 0,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Write a chunk of specimens into one HDF5 file.\n",
    "    Accepts either raw subgraphs or (subgraph, full_worm) pairs.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with h5py.File(file_path, \"w\") as h5f:\n",
    "        group = h5f.create_group(\"specimens\")\n",
    "        specimen_id = start_idx\n",
    "\n",
    "        for entry in specimens:\n",
    "            if isinstance(entry, tuple) and len(entry) == 2:\n",
    "                subgraph, full_worm = entry\n",
    "            else:\n",
    "                subgraph = np.asarray(entry, dtype=np.float32)\n",
    "                full_worm = subgraph\n",
    "\n",
    "            if subgraph.ndim != 2 or subgraph.shape[1] != 4:\n",
    "                raise ValueError(f\"subgraph must be [N,4], got {subgraph.shape}\")\n",
    "            if full_worm.ndim != 2 or full_worm.shape[1] != 4:\n",
    "                raise ValueError(f\"full_worm must be [M,4], got {full_worm.shape}\")\n",
    "\n",
    "            specimen_group = group.create_group(f\"{file_prefix}_{specimen_id:06d}\")\n",
    "            specimen_group.create_dataset(\"subgraph\", data=subgraph, compression=\"gzip\")\n",
    "            specimen_group.create_dataset(\"full_worm\", data=full_worm, compression=\"gzip\")\n",
    "\n",
    "            specimen_id += 1\n",
    "\n",
    "    return specimen_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded530c-0390-4149-8867-4016c18363ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in SHIFT_STEPS:\n",
    "    sliced_samples: List[Tuple[np.ndarray, np.ndarray]] = []  # (subgraph, full_worm)\n",
    "    per_worm_selected: List[int] = []\n",
    "    per_worm_slice_counts: List[List[int]] = []\n",
    "\n",
    "    for canonical_ids, coords in all_worms:\n",
    "        coords_arr = np.asarray(coords, dtype=np.float32)\n",
    "        ids_arr = np.asarray(canonical_ids, dtype=np.int64)\n",
    "\n",
    "        if coords_arr.ndim != 2 or coords_arr.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected coords to be [N, 3], got {coords_arr.shape}\")\n",
    "        \n",
    "        crop_side = RNG.choice((\"positive\", \"negative\"))\n",
    "        crop_fraction = RNG.uniform(*CROP_FRACTION_RANGE)\n",
    "        random_state = np.random.default_rng(RNG.integers(0, 2**32))\n",
    "\n",
    "        selected_idx, per_slice_counts, projected_coords = get_slice_indices(\n",
    "            coords_arr,\n",
    "            n_slices=NUM_SLICES,\n",
    "            slice_thickness=SLICE_THICKNESS,\n",
    "            shift=shift,\n",
    "            crop_axis=CROP_AXIS,\n",
    "            crop_side=crop_side,\n",
    "            crop_fraction=crop_fraction,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        if selected_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        per_worm_selected.append(len(selected_idx))\n",
    "        per_worm_slice_counts.append(per_slice_counts)\n",
    "\n",
    "        sliced_ids = ids_arr[selected_idx]             # keep canonical IDs in the slice order\n",
    "        sample = np.zeros((len(selected_idx), 4), dtype=np.float32)\n",
    "        sample[:, 0] = sliced_ids.astype(np.float32)   # column 0: canonical IDs\n",
    "        sample[:, 1:] = projected_coords               # columns 1–3: 3-D coordinates lying on the slice planes\n",
    "\n",
    "        full_worm_sample = np.zeros((len(ids_arr), 4), dtype=np.float32)\n",
    "        full_worm_sample[:, 0] = ids_arr.astype(np.float32)\n",
    "        full_worm_sample[:, 1:] = coords_arr\n",
    "        \n",
    "        sliced_samples.append((sample, full_worm_sample))\n",
    "\n",
    "        print(\n",
    "            f\"Sliced worm (len={len(ids_arr)}) @ shift {shift:.4f}: \"\n",
    "            f\"{len(selected_idx)} nuclei | per-slice counts = {per_slice_counts}\"\n",
    "            f\"| crop_side={crop_side} | crop_fraction={crop_fraction:.3f}\"\n",
    "        )\n",
    "\n",
    "    if not sliced_samples:\n",
    "        print(f\"No samples produced for shift {shift:.4f}; skipping file output.\")\n",
    "        continue\n",
    "\n",
    "    output_dir = subgraph_output_directory / f\"sliced_subgraphs_shift_{shift:.3f}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        sliced_samples,\n",
    "        output_dir=output_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "    print(f\"Saved {len(sliced_samples)} sliced subgraphs to {output_dir}\")\n",
    "\n",
    "    avg_selected = float(np.mean(per_worm_selected))\n",
    "    std_selected = float(np.std(per_worm_selected))\n",
    "\n",
    "    slice_matrix = np.asarray(per_worm_slice_counts, dtype=float)\n",
    "    avg_slice_counts = slice_matrix.mean(axis=0)\n",
    "    std_slice_counts = slice_matrix.std(axis=0)\n",
    "    avg_per_slice_overall = float(avg_slice_counts.mean())\n",
    "\n",
    "    stats_path = output_dir / \"slice_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"shift: {shift:.4f}\\n\")\n",
    "        fh.write(f\"num_worms: {len(per_worm_selected)}\\n\")\n",
    "        fh.write(f\"avg_total_nuclei: {avg_selected:.2f}\\n\")\n",
    "        fh.write(f\"std_total_nuclei: {std_selected:.2f}\\n\")\n",
    "        fh.write(f\"avg_per_slice_overall: {avg_per_slice_overall:.2f}\\n\")\n",
    "        fh.write(\"avg_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in avg_slice_counts) + \"\\n\")\n",
    "        fh.write(\"std_per_slice_counts: \" +\n",
    "                 \", \".join(f\"{v:.2f}\" for v in std_slice_counts) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c2ef87-1558-4c57-8d68-8bb1d30ae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimens_per_file = 2**14\n",
    "rng_seed = 42\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "min_size = 111\n",
    "max_size = 117\n",
    "subgraph_output_directory = Path(\"/fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40\")\n",
    "subgraph_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "sample_count = 12\n",
    "num_datasets = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f24ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2400 subgraphs (avg size 113.99 ± 2.00) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_00\n",
      "Saved 2400 subgraphs (avg size 113.99 ± 2.00) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_01\n",
      "Saved 2400 subgraphs (avg size 113.97 ± 1.99) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_02\n",
      "Saved 2400 subgraphs (avg size 113.99 ± 1.99) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_03\n",
      "Saved 2400 subgraphs (avg size 113.97 ± 2.02) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_04\n",
      "Saved 2400 subgraphs (avg size 114.06 ± 2.00) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_05\n",
      "Saved 2400 subgraphs (avg size 113.98 ± 2.02) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_06\n",
      "Saved 2400 subgraphs (avg size 114.01 ± 1.99) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_07\n",
      "Saved 2400 subgraphs (avg size 114.07 ± 1.99) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_08\n",
      "Saved 2400 subgraphs (avg size 114.04 ± 1.99) to /fs/pool/pool-mlsb/bulat/Wormologist/mispredictions/random/comparison_to_real_test_set_40/dataset_09\n"
     ]
    }
   ],
   "source": [
    "for dataset_idx in range(num_datasets):\n",
    "    # Optional: vary seed per dataset for reproducibility\n",
    "    \n",
    "    specimens = []        # will hold (subgraph, full_worm) tuples\n",
    "    subgraph_sizes = []\n",
    "\n",
    "    for answer, coords in all_worms:\n",
    "        answer = np.asarray(answer, dtype=np.int64)          # [N]\n",
    "        coords = np.asarray(coords, dtype=np.float32)        # [N, 3]\n",
    "        nodes = len(answer)\n",
    "        if nodes == 0:\n",
    "            continue\n",
    "\n",
    "        # Build full_worm once per worm\n",
    "        full_worm = np.zeros((nodes, 4), dtype=np.float32)\n",
    "        full_worm[:, 0] = answer.astype(np.float32)          # canonical_id\n",
    "        full_worm[:, 1:] = coords                            # x,y,z\n",
    "\n",
    "        # Sample subgraphs from this worm\n",
    "        for _ in range(sample_count):\n",
    "            subgraph_size = int(rng.integers(min_size, max_size + 1))\n",
    "            sample_size = min(subgraph_size, nodes)\n",
    "            subgraph_sizes.append(sample_size)\n",
    "\n",
    "            sampled_indices = rng.choice(nodes, size=sample_size, replace=False)\n",
    "\n",
    "            subgraph = np.zeros((sample_size, 4), dtype=np.float32)\n",
    "            subgraph[:, 0] = answer[sampled_indices].astype(np.float32)\n",
    "            subgraph[:, 1:] = coords[sampled_indices]\n",
    "\n",
    "            # Pair the subgraph with the full worm\n",
    "            specimens.append((subgraph, full_worm))\n",
    "\n",
    "    subgraph_dir = subgraph_output_directory / f\"dataset_{dataset_idx:02d}\"\n",
    "    convert_specimens_to_hdf5(\n",
    "        specimens,\n",
    "        output_dir=subgraph_dir,\n",
    "        specimens_per_file=specimens_per_file,\n",
    "    )\n",
    "\n",
    "    avg_size = float(np.mean(subgraph_sizes)) if subgraph_sizes else float(\"nan\")\n",
    "    std_size = float(np.std(subgraph_sizes)) if subgraph_sizes else float(\"nan\")\n",
    "    min_size_obs = int(np.min(subgraph_sizes)) if subgraph_sizes else 0\n",
    "    max_size_obs = int(np.max(subgraph_sizes)) if subgraph_sizes else 0\n",
    "\n",
    "    print(\n",
    "        f\"Saved {len(specimens)} subgraphs \"\n",
    "        f\"(avg size {avg_size:.2f} ± {std_size:.2f}) to {subgraph_dir}\"\n",
    "    )\n",
    "\n",
    "    stats_path = subgraph_dir / \"subgraph_stats.txt\"\n",
    "    with stats_path.open(\"w\") as fh:\n",
    "        fh.write(f\"dataset_index: {dataset_idx}\\n\")\n",
    "        fh.write(f\"num_samples: {len(specimens)}\\n\")\n",
    "        fh.write(f\"size_range_config: [{min_size}, {max_size}]\\n\")\n",
    "        fh.write(f\"size_min_observed: {min_size_obs}\\n\")\n",
    "        fh.write(f\"size_max_observed: {max_size_obs}\\n\")\n",
    "        fh.write(f\"size_mean: {avg_size:.2f}\\n\")\n",
    "        fh.write(f\"size_std: {std_size:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991157c-c11f-47a8-8ae8-4ac9df32e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
